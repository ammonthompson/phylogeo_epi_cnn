{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c44607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this script was probably combined with cqr_uq_extant.ipynb...check\n",
    "\n",
    "\n",
    "import os, shutil\n",
    "import io\n",
    "from io import StringIO\n",
    "import re\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "from keras import models\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import scipy.stats as sp\n",
    "from scipy.stats import kde\n",
    "import importlib as im\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "\n",
    "# my utilities\n",
    "import cnn_utilities as cn\n",
    "import uq_utilities as uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b94ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model and normalization values\n",
    "my_model = models.load_model(\"../saved_models/train_extant_R0_sampleRate_migrationRate.hdf5\")\n",
    "\n",
    "mean_sd = pd.read_csv(\"../saved_models/train_extant_normalization_label_mean_sd.csv\", \n",
    "                     index_col = 0).to_numpy()\n",
    "\n",
    "train_means = mean_sd[0,:]\n",
    "train_sd = mean_sd[1,:]\n",
    "train_aux_priors_means = train_means[3:,]\n",
    "train_aux_priors_sd = train_sd[3:,]\n",
    "\n",
    "num_locs = 5\n",
    "max_tips = 502\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "acbb3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation data\n",
    "extant_data = pd.read_csv(\"../data_files/extant_training_set_0to40.cblv.csv\", \n",
    "                   header =None, error_bad_lines = False, index_col = 0).to_numpy()\n",
    "\n",
    "extant_labels = pd.read_csv(\"../data_files/extant_training_set_0to40_labels.csv\",\n",
    "                    header = None, error_bad_lines = False).to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6f1de2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extant_data_mu_ntips = pd.read_table(\"data_files/labels_and_preds/extant_training_set_0to40_param_values_numtips_propsamp.txt\", \n",
    "                                  header = 0).to_numpy()\n",
    "extant_data_mu_ntips = np.column_stack((extant_data_mu_ntips[:,0], extant_data_mu_ntips[:,1]/extant_data_mu_ntips[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "42957377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# compute and gather auxilliary prior data ##\n",
    "##############################################\n",
    "extant_subsample_prop = extant_data[:,(max_tips-1) * 7]\n",
    "phylocomp_mu = extant_data[:,(max_tips - 3) * 7]\n",
    "extant_num_tips = cn.get_num_tips(extant_data)\n",
    "\n",
    "aux_phylocomp = np.vstack((phylocomp_mu, extant_subsample_prop, extant_num_tips,\n",
    "                          extant_labels[:,8], extant_labels[:,9])).transpose()\n",
    "\n",
    "norm_aux_phylocomp = cn.normalize(aux_phylocomp, (train_aux_priors_means, train_aux_priors_sd))\n",
    "\n",
    "\n",
    "# create input tensors\n",
    "extant_treeLocation_tensor, extant_prior_tensor = cn.create_data_tensors(data = extant_data, \n",
    "                                                                                    mu = norm_aux_phylocomp[:,0],\n",
    "                                                                                    subsample_prop = norm_aux_phylocomp[:,1],\n",
    "                                                                                    num_tips = norm_aux_phylocomp[:,2],\n",
    "                                                                                    tmrca = norm_aux_phylocomp[:,3],\n",
    "                                                                                    mean_bl = norm_aux_phylocomp[:,4],\n",
    "                                                                                    num_locs = num_locs,\n",
    "                                                                                    max_tips = max_tips,\n",
    "                                                                                    cblv_contains_mu_rho = True)\n",
    "\n",
    "# PREDICT R0, sample rate, migration rate\n",
    "extant_normalized_preds = my_model.predict([extant_treeLocation_tensor, extant_prior_tensor])\n",
    "\n",
    "# reversing normalization\n",
    "cnn_extant_preds = cn.denormalize(extant_normalized_preds, train_means[0:3], train_sd[0:3])\n",
    "log_preds = cnn_extant_preds\n",
    "cnn_extant_preds = np.exp(cnn_extant_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af05e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############ checking coverage #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "90892626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 is done.   time = 36.015483140945435\n",
      "0.1 is done.   time = 36.108850955963135\n",
      "0.25 is done.   time = 35.96851062774658\n",
      "0.5 is done.   time = 36.21351146697998\n",
      "0.75 is done.   time = 35.48666763305664\n",
      "0.9 is done.   time = 35.67466711997986\n",
      "0.95 is done.   time = 36.49275612831116\n"
     ]
    }
   ],
   "source": [
    "im.reload(uq)\n",
    "# make dictionaries containing interval estimating functions\n",
    "# the keys should be the quantile number\n",
    "# the values should be (lower_q_fun, upper_q_fun)\n",
    "train_dat = np.column_stack((log_preds[:,0:3], extant_data_mu_ntips))\n",
    "\n",
    "nn = log_preds.shape[0]\n",
    "\n",
    "train_R0_cpi_fun = {}\n",
    "train_delta_cpi_fun = {}\n",
    "train_m_cpi_fun = {}\n",
    "\n",
    "inner_q = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "# inner_q = [0.95]\n",
    "grid_points = 20\n",
    "for q in inner_q:\n",
    "    start_time = time.time()\n",
    "\n",
    "    m, lower_q, upper_q = uq.get_CPI(train_dat[0:nn,[0,3,4]],\n",
    "                                                   np.log(extant_labels[0:nn,5]), \n",
    "                                                   frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
    "    train_R0_cpi_fun[q] = (lower_q, upper_q)\n",
    "    \n",
    "    m, lower_q, upper_q = uq.get_CPI(train_dat[0:nn,[1,3,4]],\n",
    "                                               np.log(extant_labels[0:nn,6]), \n",
    "                                               frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
    "    train_delta_cpi_fun[q] = (lower_q, upper_q)\n",
    "    \n",
    "    m, lower_q, upper_q = uq.get_CPI(train_dat[0:nn,[2,3,4]],\n",
    "                                               np.log(extant_labels[0:nn,7]), \n",
    "                                               frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
    "    train_m_cpi_fun[q] = (lower_q, upper_q)\n",
    "    \n",
    "    print(str(q) + \" is done.   time = \" + str(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ae268fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 5.06130967910253\n",
      "0.1 finished: 10.033556141313637\n",
      "0.25 finished: 25.12392381946256\n",
      "0.5 finished: 50.49614509207697\n",
      "0.75 finished: 75.54719900681019\n",
      "0.9 finished: 90.44594582437453\n",
      "0.95 finished: 95.0646383043803\n",
      "0.05 finished: 5.052313394568044\n",
      "0.1 finished: 10.22247811653787\n",
      "0.25 finished: 25.44419154889031\n",
      "0.5 finished: 50.891081983140964\n",
      "0.75 finished: 76.33167501821747\n",
      "0.9 finished: 90.87057045440233\n",
      "0.95 finished: 95.36781309319251\n",
      "0.05 finished: 4.984841260559389\n",
      "0.1 finished: 10.083035706253316\n",
      "0.25 finished: 25.298451739431616\n",
      "0.5 finished: 50.57711165288735\n",
      "0.75 finished: 76.07438128053114\n",
      "0.9 finished: 90.91015410635409\n",
      "0.95 finished: 95.46857147997876\n"
     ]
    }
   ],
   "source": [
    "# check the train set\n",
    "inner_q = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "i=0\n",
    "uq_cal_R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   train_dat[:,[i,3,4]], (extant_labels[:,5+i]), quantiles = inner_q, q_fun = train_R0_cpi_fun)\n",
    "i=1\n",
    "uq_cal_delta_coverage = uq.make_coverage_set(None, None,  \n",
    "                                   train_dat[:,[i,3,4]], (extant_labels[:,5+i]), quantiles = inner_q, q_fun = train_delta_cpi_fun)\n",
    "i=2\n",
    "uq_cal_m_coverage = uq.make_coverage_set(None, None,  \n",
    "                                   train_dat[:,[i,3,4]], (extant_labels[:,5+i]), quantiles = inner_q, q_fun = train_m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "af98c0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111157, 5)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "537fd9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 3697\n"
     ]
    }
   ],
   "source": [
    "# calibration data set\n",
    "uq_cal_data = pd.read_csv(\"data_files/labels_and_preds/uq_calibration_sets_0to40.cblv.csv\",\n",
    "                            header =None, error_bad_lines = False, index_col = 0).to_numpy()\n",
    "uq_cal_labels = pd.read_table(\"data_files/labels_and_preds/uq_calibration_sets_0to40_labels.tsv\", header = 0).to_numpy()\n",
    "uq_cal_mutips = pd.read_table(\"data_files/labels_and_preds/uq_calibration_sets_0to40_numtips_propsamp.txt\", header = 0).to_numpy()\n",
    "uq_cal_mutips = np.column_stack((uq_cal_mutips[:,0], uq_cal_mutips[:,1]/uq_cal_mutips[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "bf9bbdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "uq_cal_subsample_prop = uq_cal_data[:,(max_tips-1) * 7]\n",
    "uq_cal_mu = uq_cal_data[:,(max_tips - 3) * 7]\n",
    "uq_cal_num_tips = cn.get_num_tips(uq_cal_data)\n",
    "\n",
    "aux_uq_cal = np.vstack((uq_cal_mu, uq_cal_subsample_prop, uq_cal_num_tips,\n",
    "                          uq_cal_labels[:,8], uq_cal_labels[:,9])).transpose()\n",
    "\n",
    "norm_aux_uq_cal = cn.normalize(aux_uq_cal, (train_aux_priors_means, train_aux_priors_sd))\n",
    "\n",
    "# create input tensors\n",
    "aux_uq_cal_treeLocation_tensor, aux_uq_cal_prior_tensor = cn.create_data_tensors(data = uq_cal_data, \n",
    "                                                                                    mu = norm_aux_uq_cal[:,0],\n",
    "                                                                                    subsample_prop = norm_aux_uq_cal[:,1],\n",
    "                                                                                    num_tips = norm_aux_uq_cal[:,2],\n",
    "                                                                                    tmrca = norm_aux_uq_cal[:,3],\n",
    "                                                                                    mean_bl = norm_aux_uq_cal[:,4],\n",
    "                                                                                    num_locs = num_locs,\n",
    "                                                                                    max_tips = max_tips,\n",
    "                                                                                    cblv_contains_mu_rho = True)\n",
    "\n",
    "# PREDICT R0, sample rate, migration rate\n",
    "uq_cal_normalized_preds = my_model.predict([aux_uq_cal_treeLocation_tensor, aux_uq_cal_prior_tensor])\n",
    "\n",
    "# reversing normalization\n",
    "cnn_uq_cal_preds = cn.denormalize(uq_cal_normalized_preds, train_means[0:3], train_sd[0:3])\n",
    "cnn_uq_cal_log_preds = np.column_stack((cnn_uq_cal_preds, uq_cal_mutips))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "b6a0161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# randomize the order\n",
    "rand_idx = np.arange(uq_cal_labels.shape[0])\n",
    "np.random.shuffle(rand_idx)\n",
    "cnn_uq_cal_log_preds = cnn_uq_cal_log_preds[rand_idx,:]\n",
    "uq_cal_labels = uq_cal_labels[rand_idx,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "80f31cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split calibration prediction dat for conformal prediction interval estimation and validation\n",
    "uq_num_val = 5000\n",
    "\n",
    "# for measuring resulting coverage\n",
    "cal_val_preds = cnn_uq_cal_log_preds[0:uq_num_val,:]\n",
    "cal_val_labels = uq_cal_labels[0:uq_num_val,:]\n",
    "\n",
    "# for creating UQ\n",
    "cal_preds = cnn_uq_cal_log_preds[uq_num_val:,:]\n",
    "cal_labels = uq_cal_labels[uq_num_val:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "4ba19205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'uq_utilities' from 'C:\\\\Users\\\\ammon_work\\\\Desktop\\\\RESEARCH_PROJECTS\\\\phylogeo_epi_cnn\\\\neural_network_dev\\\\uq_and_adequacy\\\\uq_utilities.py'>"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.reload(uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc733f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-456-b6c3c9a6a91a>\", line 19, in <module>\n",
      "    frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
      "  File \"C:\\Users\\ammon_work\\Desktop\\RESEARCH_PROJECTS\\phylogeo_epi_cnn\\neural_network_dev\\uq_and_adequacy\\uq_utilities.py\", line 106, in get_CPI\n",
      "    local_lower_q[i, j, k] = point[0] + np.quantile(residuals, rlq)\n",
      "  File \"<__array_function__ internals>\", line 6, in quantile\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3845, in quantile\n",
      "    a, q, axis, out, overwrite_input, interpolation, keepdims)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3853, in _quantile_unchecked\n",
      "    interpolation=interpolation)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3429, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3954, in _quantile_ureduce_func\n",
      "    ap.partition(concatenate((indices_below, indices_above)), axis=axis)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-456-b6c3c9a6a91a>\", line 19, in <module>\n",
      "    frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
      "  File \"C:\\Users\\ammon_work\\Desktop\\RESEARCH_PROJECTS\\phylogeo_epi_cnn\\neural_network_dev\\uq_and_adequacy\\uq_utilities.py\", line 106, in get_CPI\n",
      "    local_lower_q[i, j, k] = point[0] + np.quantile(residuals, rlq)\n",
      "  File \"<__array_function__ internals>\", line 6, in quantile\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3845, in quantile\n",
      "    a, q, axis, out, overwrite_input, interpolation, keepdims)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3853, in _quantile_unchecked\n",
      "    interpolation=interpolation)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3429, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3954, in _quantile_ureduce_func\n",
      "    ap.partition(concatenate((indices_below, indices_above)), axis=axis)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-456-b6c3c9a6a91a>\", line 19, in <module>\n",
      "    frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
      "  File \"C:\\Users\\ammon_work\\Desktop\\RESEARCH_PROJECTS\\phylogeo_epi_cnn\\neural_network_dev\\uq_and_adequacy\\uq_utilities.py\", line 106, in get_CPI\n",
      "    local_lower_q[i, j, k] = point[0] + np.quantile(residuals, rlq)\n",
      "  File \"<__array_function__ internals>\", line 6, in quantile\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3845, in quantile\n",
      "    a, q, axis, out, overwrite_input, interpolation, keepdims)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3853, in _quantile_unchecked\n",
      "    interpolation=interpolation)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3429, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3954, in _quantile_ureduce_func\n",
      "    ap.partition(concatenate((indices_below, indices_above)), axis=axis)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\ammon_work\\Anaconda3\\envs\\tf-keras-gpu\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "# make dictionaries containing interval estimating functions\n",
    "# the keys should be the quantile number\n",
    "# the values should be (lower_q_fun, upper_q_fun)\n",
    "\n",
    "nn = cal_preds.shape[0]\n",
    "\n",
    "R0_cpi_fun = {}\n",
    "delta_cpi_fun = {}\n",
    "m_cpi_fun = {}\n",
    "\n",
    "inner_q = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "# inner_q = [0.85, 0.9, 0.95]\n",
    "grid_points = 50\n",
    "for q in inner_q:\n",
    "    start_time = time.time()\n",
    "\n",
    "    m, lower_q, upper_q = uq.get_CPI(cal_preds[0:nn,[0,3,4]],\n",
    "                                                   np.log(cal_labels[0:nn,0]), \n",
    "                                                   frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
    "    R0_cpi_fun[q] = (lower_q, upper_q)\n",
    "    \n",
    "    m, lower_q, upper_q = uq.get_CPI(cal_preds[0:nn,[1,3,4]],\n",
    "                                               np.log(cal_labels[0:nn,1]), \n",
    "                                               frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
    "    delta_cpi_fun[q] = (lower_q, upper_q)\n",
    "    \n",
    "    m, lower_q, upper_q = uq.get_CPI(cal_preds[0:nn,[2,3,4]],\n",
    "                                               np.log(cal_labels[0:nn,2]), \n",
    "                                               frac=0.05, inner_quantile=q, grid_points = grid_points)\n",
    "    m_cpi_fun[q] = (lower_q, upper_q)\n",
    "    \n",
    "    print(str(q) + \" is done.   time = \" + str(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "f067e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 5.056236700780221\n",
      "0.1 finished: 9.976142005729603\n",
      "0.25 finished: 25.009441870319364\n",
      "0.5 finished: 50.13126502639118\n",
      "0.75 finished: 75.39678884293333\n",
      "0.9 finished: 90.33060363488978\n",
      "0.95 finished: 94.96955572545804\n",
      "0.05 finished: 5.077423336618797\n",
      "0.1 finished: 10.129975405079266\n",
      "0.25 finished: 25.29407971702024\n",
      "0.5 finished: 50.73001777835094\n",
      "0.75 finished: 76.32623734559087\n",
      "0.9 finished: 91.01594524636373\n",
      "0.95 finished: 95.5029062537422\n",
      "0.05 finished: 5.012021112943192\n",
      "0.1 finished: 9.996407483488241\n",
      "0.25 finished: 25.134719369190943\n",
      "0.5 finished: 50.5743420628414\n",
      "0.75 finished: 76.16871931392146\n",
      "0.9 finished: 91.04357998876186\n",
      "0.95 finished: 95.62357796221409\n"
     ]
    }
   ],
   "source": [
    "# check calibration data\n",
    "i=0\n",
    "uq_cal_R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cal_preds[:,[i,3,4]], cal_labels[:,i], quantiles = inner_q, q_fun = R0_cpi_fun)\n",
    "i=1\n",
    "uq_cal_delta_coverage = uq.make_coverage_set(None, None,  \n",
    "                                   cal_preds[:,[i,3,4]], cal_labels[:,i], quantiles = inner_q, q_fun = delta_cpi_fun)\n",
    "i=2\n",
    "uq_cal_m_coverage = uq.make_coverage_set(None, None,  \n",
    "                                   cal_preds[:,[i,3,4]], cal_labels[:,i], quantiles = inner_q, q_fun = m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "6958e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 5.06\n",
      "0.1 finished: 9.86\n",
      "0.25 finished: 25.44\n",
      "0.5 finished: 50.44\n",
      "0.75 finished: 74.4\n",
      "0.9 finished: 89.98\n",
      "0.95 finished: 95.39999999999999\n",
      "0.05 finished: 4.74\n",
      "0.1 finished: 9.58\n",
      "0.25 finished: 24.36\n",
      "0.5 finished: 50.22\n",
      "0.75 finished: 76.62\n",
      "0.9 finished: 91.03999999999999\n",
      "0.95 finished: 96.12\n",
      "0.05 finished: 5.1\n",
      "0.1 finished: 10.22\n",
      "0.25 finished: 25.52\n",
      "0.5 finished: 50.2\n",
      "0.75 finished: 75.98\n",
      "0.9 finished: 90.53999999999999\n",
      "0.95 finished: 95.36\n"
     ]
    }
   ],
   "source": [
    "# check cal hold-out validation data\n",
    "i=0\n",
    "uq_cal_R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cal_val_preds[:,[i,3,4]], cal_val_labels[:,i], quantiles = inner_q, q_fun = R0_cpi_fun)\n",
    "i=1\n",
    "uq_cal_delta_coverage = uq.make_coverage_set(None, None,  \n",
    "                                   cal_val_preds[:,[i,3,4]], cal_val_labels[:,i], quantiles = inner_q, q_fun = delta_cpi_fun)\n",
    "i=2\n",
    "uq_cal_m_coverage = uq.make_coverage_set(None, None,  \n",
    "                                   cal_val_preds[:,[i,3,4]], cal_val_labels[:,i], quantiles = inner_q, q_fun = m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "24520e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "df_caltest_coverage = pd.DataFrame(np.transpose(np.vstack((uq_cal_R0_coverage, uq_cal_delta_coverage, uq_cal_m_coverage))), \n",
    "                                            columns = [\"R0\", \"sample_rate\", \"migration_rate\"], \n",
    "                               index = [\"5\", \"10\", \"25\", \"50\", \"75\", \"90\", \"95\"])\n",
    "df_caltest_coverage.to_csv('../data_files/caltest_coverage.tsv', sep = \"\\t\", index = True)\n",
    "\n",
    "df_caltest_labels = pd.DataFrame(cal_val_labels[:,0:3], columns = ['R0', 'sample_rate', 'migration_rate'])\n",
    "df_caltest_labels.to_csv(\"output/caltest_labels.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "# write 95% quantiles to file\n",
    "caltest_R0_95_q = np.array([R0_cpi_fun[0.95][0](cal_val_preds[:,[0,3,4]]), R0_cpi_fun[0.95][1](cal_val_preds[:,[0,3,4]])]).transpose()\n",
    "caltest_delta_95_q = np.array([delta_cpi_fun[0.95][0](cal_val_preds[:,[1,3,4]]), delta_cpi_fun[0.95][1](cal_val_preds[:,[1,3,4]])]).transpose()\n",
    "caltest_m_95_q = np.array([m_cpi_fun[0.95][0](cal_val_preds[:,[2,3,4]]), m_cpi_fun[0.95][1](cal_val_preds[:,[2,3,4]])]).transpose()\n",
    "\n",
    "df_caltest_95q = pd.DataFrame(np.hstack((caltest_R0_95_q, caltest_delta_95_q, caltest_m_95_q)),\n",
    "                         columns = [\"R0_lq\", \"R0_uq\", \"delta_lq\", \"delta_uq\", \"m_lq\", \"m_uq\"])\n",
    "df_caltest_95q.to_csv('output/caltest_95q.tsv', sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "b4034697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 7.246376811594203\n",
      "0.1 finished: 8.695652173913043\n",
      "0.25 finished: 28.26086956521739\n",
      "0.5 finished: 59.42028985507246\n",
      "0.75 finished: 82.6086956521739\n",
      "0.9 finished: 92.02898550724638\n",
      "0.95 finished: 96.37681159420289\n",
      "0.05 finished: 6.521739130434782\n",
      "0.1 finished: 10.144927536231885\n",
      "0.25 finished: 31.15942028985507\n",
      "0.5 finished: 60.14492753623188\n",
      "0.75 finished: 79.71014492753623\n",
      "0.9 finished: 92.7536231884058\n",
      "0.95 finished: 97.10144927536231\n",
      "0.05 finished: 5.797101449275362\n",
      "0.1 finished: 10.869565217391305\n",
      "0.25 finished: 25.36231884057971\n",
      "0.5 finished: 48.55072463768116\n",
      "0.75 finished: 73.18840579710145\n",
      "0.9 finished: 92.7536231884058\n",
      "0.95 finished: 97.10144927536231\n"
     ]
    }
   ],
   "source": [
    "# coverage experiment 0 (true model dataset)\n",
    "im.reload(uq)\n",
    "\n",
    "### coverage of CI for test dataset\n",
    "cnn_phylocomp_preds = pd.read_table(\"data_files/labels_and_preds/extant_cnn_preds.tsv\", header = 0).to_numpy()\n",
    "cnn_phylocomp_labels = pd.read_table(\"data_files/labels_and_preds/extant_labels.tsv\", header = 0).to_numpy()\n",
    "cnn_phylocomp_mutips = pd.read_table(\"data_files/labels_and_preds/extant_phylocomp_param_values_numtips_propsamp.txt\", header = 0).to_numpy()\n",
    "\n",
    "cnn_phylocomp_preds = np.column_stack((np.log(cnn_phylocomp_preds[:,0:3]), cnn_phylocomp_mutips[:,0], cnn_phylocomp_mutips[:,1]/cnn_phylocomp_mutips[:,2]))\n",
    "\n",
    "# phylocomp true model\n",
    "\n",
    "# compute coverages\n",
    "i=0\n",
    "R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_phylocomp_preds[:,[i,3,4]], cnn_phylocomp_labels[:,i], quantiles = inner_q, q_fun = R0_cpi_fun)\n",
    "i=1\n",
    "delta_coverage = uq.make_coverage_set(None, None,  \n",
    "                                   cnn_phylocomp_preds[:,[i,3,4]], cnn_phylocomp_labels[:,i], quantiles = inner_q, q_fun = delta_cpi_fun)\n",
    "i=2\n",
    "m_coverage = uq.make_coverage_set(None, None,  \n",
    "                                   cnn_phylocomp_preds[:,[i,3,4]], cnn_phylocomp_labels[:,i], quantiles = inner_q, q_fun = m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "d2344041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "df_cnn_coverage = pd.DataFrame(np.transpose(np.vstack((R0_coverage, delta_coverage, m_coverage))), \n",
    "                                            columns = [\"R0\", \"sample_rate\", \"migration_rate\"], \n",
    "                               index = [\"5\", \"10\", \"25\", \"50\", \"75\", \"90\", \"95\"])\n",
    "df_cnn_coverage.to_csv('../data_files/fuck_cnn_coverage.tsv', sep = \"\\t\", index = True)\n",
    "\n",
    "# write 95% quantiles to file\n",
    "phylocomp_R0_95_q = np.array([R0_cpi_fun[0.95][0](cnn_phylocomp_preds[:,[0,3,4]]), R0_cpi_fun[0.95][1](cnn_phylocomp_preds[:,[0,3,4]])]).transpose()\n",
    "phylocomp_delta_95_q = np.array([delta_cpi_fun[0.95][0](cnn_phylocomp_preds[:,[1,3,4]]), delta_cpi_fun[0.95][1](cnn_phylocomp_preds[:,[1,3,4]])]).transpose()\n",
    "phylocomp_m_95_q = np.array([m_cpi_fun[0.95][0](cnn_phylocomp_preds[:,[2,3,4]]), m_cpi_fun[0.95][1](cnn_phylocomp_preds[:,[2,3,4]])]).transpose()\n",
    "\n",
    "df_cnn_95q = pd.DataFrame(np.hstack((phylocomp_R0_95_q, phylocomp_delta_95_q, phylocomp_m_95_q)),\n",
    "                         columns = [\"R0_lq\", \"R0_uq\", \"delta_lq\", \"delta_uq\", \"m_lq\", \"m_uq\"])\n",
    "df_cnn_95q.to_csv('output/fuck_phylocomp_cnn_95q.tsv', sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "ba667542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure coverage of predictions on out-of-model test data (misspecified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "ca1883c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 0.0\n",
      "0.1 finished: 0.0\n",
      "0.25 finished: 0.0\n",
      "0.5 finished: 8.60215053763441\n",
      "0.75 finished: 19.35483870967742\n",
      "0.9 finished: 33.33333333333333\n",
      "0.95 finished: 41.935483870967744\n",
      "0.05 finished: 4.301075268817205\n",
      "0.1 finished: 8.60215053763441\n",
      "0.25 finished: 24.731182795698924\n",
      "0.5 finished: 43.01075268817204\n",
      "0.75 finished: 73.11827956989248\n",
      "0.9 finished: 86.02150537634408\n",
      "0.95 finished: 91.39784946236558\n",
      "0.05 finished: 3.225806451612903\n",
      "0.1 finished: 8.60215053763441\n",
      "0.25 finished: 13.978494623655912\n",
      "0.5 finished: 25.806451612903224\n",
      "0.75 finished: 50.53763440860215\n",
      "0.9 finished: 75.26881720430107\n",
      "0.95 finished: 79.56989247311827\n"
     ]
    }
   ],
   "source": [
    "# make coverage sets for R0 misspecification experiments\n",
    "cnn_miss_r0_preds = pd.read_table(\"../output/misspec_R0_cnn_preds.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_r0_labels = pd.read_table(\"../output/misspec_R0_labels.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_r0_mutips = pd.read_table(\"data_files/labels_and_preds/extant_misspec_R0_param_values_numtips_propsamp.txt\", \n",
    "                                     header = 0).to_numpy()\n",
    "\n",
    "cnn_miss_r0_preds = np.column_stack((np.log(cnn_miss_r0_preds[:,0:3]), cnn_miss_r0_mutips[:,0], \n",
    "                                       cnn_miss_r0_mutips[:,1]/cnn_miss_r0_mutips[:,2]))\n",
    "\n",
    "\n",
    "i=0\n",
    "missR0_R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_r0_preds[:,[i,3,4]], cnn_miss_r0_labels[:,i], q_fun = R0_cpi_fun)\n",
    "i=1\n",
    "missR0_delta_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_r0_preds[:,[i,3,4]], cnn_miss_r0_labels[:,i], q_fun = delta_cpi_fun)\n",
    "i=2\n",
    "missR0_m_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_r0_preds[:,[i,3,4]], cnn_miss_r0_labels[:,i], q_fun = m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "948a1975",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 0.0\n",
      "0.1 finished: 0.0\n",
      "0.25 finished: 1.0752688172043012\n",
      "0.5 finished: 7.526881720430108\n",
      "0.75 finished: 15.053763440860216\n",
      "0.9 finished: 24.731182795698924\n",
      "0.95 finished: 35.483870967741936\n",
      "0.05 finished: 5.376344086021505\n",
      "0.1 finished: 6.451612903225806\n",
      "0.25 finished: 22.58064516129032\n",
      "0.5 finished: 37.634408602150536\n",
      "0.75 finished: 65.59139784946237\n",
      "0.9 finished: 80.64516129032258\n",
      "0.95 finished: 87.09677419354838\n",
      "0.05 finished: 2.1505376344086025\n",
      "0.1 finished: 5.376344086021505\n",
      "0.25 finished: 12.903225806451612\n",
      "0.5 finished: 23.655913978494624\n",
      "0.75 finished: 45.16129032258064\n",
      "0.9 finished: 61.29032258064516\n",
      "0.95 finished: 73.11827956989248\n"
     ]
    }
   ],
   "source": [
    "df_missR0_cnn_coverage = pd.DataFrame(np.transpose(np.vstack((missR0_R0_coverage, missR0_delta_coverage, missR0_m_coverage))), \n",
    "                                            columns = [\"R0\", \"sample_rate\", \"migration_rate\"], \n",
    "                               index = [\"5\", \"10\", \"25\", \"50\", \"75\", \"90\", \"95\"])\n",
    "df_missR0_cnn_coverage.to_csv('../data_files/missR0_cnn_coverage.tsv', sep = \"\\t\", index = True)\n",
    "\n",
    "# write 95% quantiles to file\n",
    "missR0_R0_95_q = np.array([R0_cpi_fun[0.95][0](cnn_miss_r0_preds[:,[0,3,4]]), R0_cpi_fun[0.95][1](cnn_miss_r0_preds[:,[0,3,4]])]).transpose()\n",
    "missR0_delta_95_q = np.array([delta_cpi_fun[0.95][0](cnn_miss_r0_preds[:,[1,3,4]]), delta_cpi_fun[0.95][1](cnn_miss_r0_preds[:,[1,3,4]])]).transpose()\n",
    "missR0_m_95_q = np.array([m_cpi_fun[0.95][0](cnn_miss_r0_preds[:,[2,3,4]]), m_cpi_fun[0.95][1](cnn_miss_r0_preds[:,[2,3,4]])]).transpose()\n",
    "\n",
    "df_cnn_95q = pd.DataFrame(np.hstack((missR0_R0_95_q, missR0_delta_95_q, missR0_m_95_q)),\n",
    "                         columns = [\"R0_lq\", \"R0_uq\", \"delta_lq\", \"delta_uq\", \"m_lq\", \"m_uq\"])\n",
    "df_cnn_95q.to_csv('output/missR0_cnn_95q.tsv', sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "e6076417",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 8.47457627118644\n",
      "0.1 finished: 12.711864406779661\n",
      "0.25 finished: 28.8135593220339\n",
      "0.5 finished: 44.06779661016949\n",
      "0.75 finished: 64.40677966101694\n",
      "0.9 finished: 83.05084745762711\n",
      "0.95 finished: 89.83050847457628\n",
      "0.05 finished: 0.0\n",
      "0.1 finished: 0.847457627118644\n",
      "0.25 finished: 5.932203389830509\n",
      "0.5 finished: 23.728813559322035\n",
      "0.75 finished: 39.83050847457627\n",
      "0.9 finished: 50.847457627118644\n",
      "0.95 finished: 61.016949152542374\n",
      "0.05 finished: 3.389830508474576\n",
      "0.1 finished: 5.084745762711865\n",
      "0.25 finished: 12.711864406779661\n",
      "0.5 finished: 32.20338983050847\n",
      "0.75 finished: 48.30508474576271\n",
      "0.9 finished: 67.79661016949152\n",
      "0.95 finished: 77.96610169491525\n"
     ]
    }
   ],
   "source": [
    "# make coverage sets for delta misspecification experiments\n",
    "\n",
    "cnn_miss_delta_preds = pd.read_table(\"../output/misspec_delta_cnn_preds.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_delta_labels = pd.read_table(\"../output/misspec_delta_labels.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_delta_mutips = pd.read_table(\"data_files/labels_and_preds/extant_misspec_delta_param_values_numtips_propsamp.txt\", \n",
    "                                     header = 0).to_numpy()\n",
    "\n",
    "cnn_miss_delta_preds = np.column_stack((np.log(cnn_miss_delta_preds[:,0:3]), cnn_miss_delta_mutips[:,0], \n",
    "                                       cnn_miss_delta_mutips[:,1]/cnn_miss_delta_mutips[:,2]))\n",
    "\n",
    "i=0\n",
    "missDelta_R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_delta_preds[:,[i,3,4]], cnn_miss_delta_labels[:,i], q_fun = R0_cpi_fun)\n",
    "i=1\n",
    "missDelta_delta_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_delta_preds[:,[i,3,4]], cnn_miss_delta_labels[:,i], q_fun = delta_cpi_fun)\n",
    "i=2\n",
    "missDelta_m_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_delta_preds[:,[i,3,4]], cnn_miss_delta_labels[:,i], q_fun = m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "45e47361",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 8.47457627118644\n",
      "0.1 finished: 12.711864406779661\n",
      "0.25 finished: 28.8135593220339\n",
      "0.5 finished: 44.06779661016949\n",
      "0.75 finished: 64.40677966101694\n",
      "0.9 finished: 82.20338983050848\n",
      "0.95 finished: 88.98305084745762\n",
      "0.05 finished: 0.0\n",
      "0.1 finished: 0.847457627118644\n",
      "0.25 finished: 5.084745762711865\n",
      "0.5 finished: 23.728813559322035\n",
      "0.75 finished: 39.83050847457627\n",
      "0.9 finished: 50.847457627118644\n",
      "0.95 finished: 60.16949152542372\n",
      "0.05 finished: 3.389830508474576\n",
      "0.1 finished: 5.084745762711865\n",
      "0.25 finished: 12.711864406779661\n",
      "0.5 finished: 32.20338983050847\n",
      "0.75 finished: 49.152542372881356\n",
      "0.9 finished: 66.94915254237289\n",
      "0.95 finished: 77.96610169491525\n"
     ]
    }
   ],
   "source": [
    "df_missDelta_missR0_cnn_coverage = pd.DataFrame(np.transpose(np.vstack((missDelta_R0_coverage, missDelta_delta_coverage, missDelta_m_coverage))), \n",
    "                                            columns = [\"R0\", \"sample_rate\", \"migration_rate\"], \n",
    "                               index = [\"5\", \"10\", \"25\", \"50\", \"75\", \"90\", \"95\"])\n",
    "df_missDelta_missR0_cnn_coverage.to_csv('../data_files/missDelta_cnn_coverage.tsv', sep = \"\\t\", index = True)\n",
    "\n",
    "# write 95% quantiles to file\n",
    "missDeltaR0_95_q = np.array([R0_cpi_fun[0.95][0](cnn_miss_delta_preds[:,[0,3,4]]), R0_cpi_fun[0.95][1](cnn_miss_delta_preds[:,[0,3,4]])]).transpose()\n",
    "missDeltadelta_95_q = np.array([delta_cpi_fun[0.95][0](cnn_miss_delta_preds[:,[1,3,4]]), delta_cpi_fun[0.95][1](cnn_miss_delta_preds[:,[1,3,4]])]).transpose()\n",
    "missDeltam_95_q = np.array([m_cpi_fun[0.95][0](cnn_miss_delta_preds[:,[2,3,4]]), m_cpi_fun[0.95][1](cnn_miss_delta_preds[:,[2,3,4]])]).transpose()\n",
    "\n",
    "df_cnn_95q = pd.DataFrame(np.hstack((missDeltaR0_95_q, missDeltadelta_95_q, missDeltam_95_q)),\n",
    "                         columns = [\"R0_lq\", \"R0_uq\", \"delta_lq\", \"delta_uq\", \"m_lq\", \"m_uq\"])\n",
    "df_cnn_95q.to_csv('output/missDeltacnn_95q.tsv', sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make coverage sets for m misspecification experiments\n",
    "cnn_miss_m_preds = pd.read_table(\"../output/misspec_migration_cnn_preds.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_m_labels = pd.read_table(\"../output/misspec_migration_labels.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_m_mutips = pd.read_table(\"data_files/labels_and_preds/extant_misspec_m_param_values_numtips_propsamp.txt\", \n",
    "                                     header = 0).to_numpy()\n",
    "\n",
    "cnn_miss_m_preds = np.column_stack((np.log(cnn_miss_m_preds[:,0:3]), cnn_miss_m_mutips[:,0], \n",
    "                                       cnn_miss_m_mutips[:,1]/cnn_miss_m_mutips[:,2]))\n",
    "\n",
    "i=0\n",
    "missM_R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_m_preds[:,[i,3,4]], cnn_miss_m_labels[:,i], q_fun = R0_cpi_fun)\n",
    "i=1\n",
    "missM_delta_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_m_preds[:,[i,3,4]], cnn_miss_m_labels[:,i], q_fun = delta_cpi_fun)\n",
    "i=2\n",
    "missM_m_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_m_preds[:,[i,3,4]], cnn_miss_m_labels[:,i], q_fun = m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1d954d3a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 3.3333333333333335\n",
      "0.1 finished: 8.88888888888889\n",
      "0.25 finished: 27.77777777777778\n",
      "0.5 finished: 48.888888888888886\n",
      "0.75 finished: 73.33333333333333\n",
      "0.9 finished: 90.0\n",
      "0.95 finished: 94.44444444444444\n",
      "0.05 finished: 3.3333333333333335\n",
      "0.1 finished: 10.0\n",
      "0.25 finished: 23.333333333333332\n",
      "0.5 finished: 47.77777777777778\n",
      "0.75 finished: 67.77777777777779\n",
      "0.9 finished: 92.22222222222223\n",
      "0.95 finished: 96.66666666666667\n",
      "0.05 finished: 4.444444444444445\n",
      "0.1 finished: 7.777777777777778\n",
      "0.25 finished: 15.555555555555555\n",
      "0.5 finished: 27.77777777777778\n",
      "0.75 finished: 54.44444444444444\n",
      "0.9 finished: 66.66666666666666\n",
      "0.95 finished: 74.44444444444444\n"
     ]
    }
   ],
   "source": [
    "df_missM_missR0_cnn_coverage = pd.DataFrame(np.transpose(np.vstack((missM_R0_coverage, missM_delta_coverage, missM_m_coverage))), \n",
    "                                            columns = [\"R0\", \"sample_rate\", \"migration_rate\"], \n",
    "                               index = [\"5\", \"10\", \"25\", \"50\", \"75\", \"90\", \"95\"])\n",
    "df_missM_missR0_cnn_coverage.to_csv('../data_files/missM_cnn_coverage.tsv', sep = \"\\t\", index = True)\n",
    "\n",
    "# write 95% quantiles to file\n",
    "missM_R0_95_q = np.array([R0_cpi_fun[0.95][0](cnn_miss_m_preds[:,[0,3,4]]), R0_cpi_fun[0.95][1](cnn_miss_m_preds[:,[0,3,4]])]).transpose()\n",
    "missM_delta_95_q = np.array([delta_cpi_fun[0.95][0](cnn_miss_m_preds[:,[1,3,4]]), delta_cpi_fun[0.95][1](cnn_miss_m_preds[:,[1,3,4]])]).transpose()\n",
    "missM_m_95_q = np.array([m_cpi_fun[0.95][0](cnn_miss_m_preds[:,[2,3,4]]), m_cpi_fun[0.95][1](cnn_miss_m_preds[:,[2,3,4]])]).transpose()\n",
    "\n",
    "df_cnn_95q = pd.DataFrame(np.hstack((missM_R0_95_q, missM_delta_95_q, missM_m_95_q)),\n",
    "                         columns = [\"R0_lq\", \"R0_uq\", \"delta_lq\", \"delta_uq\", \"m_lq\", \"m_uq\"])\n",
    "df_cnn_95q.to_csv('output/missM_cnn_95q.tsv', sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569bc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make coverage sets for numloc misspecification experiments\n",
    "\n",
    "cnn_miss_loc_preds = pd.read_table(\"../output/misspec_numloc_cnn_preds.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_loc_labels = pd.read_table(\"../output/misspec_numloc_labels.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_loc_mutips = pd.read_table(\"data_files/labels_and_preds/extant_misspec_numloc_param_values_numtips_propsamp.txt\", \n",
    "                                     header = 0).to_numpy()\n",
    "\n",
    "cnn_miss_loc_preds = np.column_stack((np.log(cnn_miss_loc_preds[:,0:3]), cnn_miss_loc_mutips[:,0], \n",
    "                                       cnn_miss_loc_mutips[:,1]/cnn_miss_loc_mutips[:,2]))\n",
    "\n",
    "i=0\n",
    "missNumLoc_R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_loc_preds[:,[i,3,4]], cnn_miss_loc_labels[:,i], q_fun = R0_cpi_fun)\n",
    "i=1\n",
    "missNumLoc_delta_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_loc_preds[:,[i,3,4]], cnn_miss_loc_labels[:,i], q_fun = delta_cpi_fun)\n",
    "i=2\n",
    "missNumLoc_m_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_loc_preds[:,[i,3,4]], cnn_miss_loc_labels[:,i], q_fun = m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "31510968",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 4.201680672268908\n",
      "0.1 finished: 6.722689075630252\n",
      "0.25 finished: 23.52941176470588\n",
      "0.5 finished: 47.05882352941176\n",
      "0.75 finished: 68.90756302521008\n",
      "0.9 finished: 86.5546218487395\n",
      "0.95 finished: 92.43697478991596\n",
      "0.05 finished: 2.5210084033613445\n",
      "0.1 finished: 6.722689075630252\n",
      "0.25 finished: 16.80672268907563\n",
      "0.5 finished: 43.69747899159664\n",
      "0.75 finished: 70.58823529411765\n",
      "0.9 finished: 86.5546218487395\n",
      "0.95 finished: 93.27731092436974\n",
      "0.05 finished: 3.361344537815126\n",
      "0.1 finished: 5.042016806722689\n",
      "0.25 finished: 15.966386554621847\n",
      "0.5 finished: 34.45378151260504\n",
      "0.75 finished: 57.98319327731093\n",
      "0.9 finished: 69.74789915966386\n",
      "0.95 finished: 75.63025210084034\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 3573\n"
     ]
    }
   ],
   "source": [
    "df_missNumLoc_cnn_coverage = pd.DataFrame(np.transpose(np.vstack((missNumLoc_R0_coverage, missNumLoc_delta_coverage, missNumLoc_m_coverage))), \n",
    "                                            columns = [\"R0\", \"sample_rate\", \"migration_rate\"], \n",
    "                               index = [\"5\", \"10\", \"25\", \"50\", \"75\", \"90\", \"95\"])\n",
    "df_missNumLoc_cnn_coverage.to_csv('../data_files/missNumLoc_cnn_coverage.tsv', sep = \"\\t\", index = True)\n",
    "\n",
    "# write 95% quantiles to file\n",
    "missNumLoc_R0_95_q = np.array([R0_cpi_fun[0.95][0](cnn_miss_loc_preds[:,[0,3,4]]), R0_cpi_fun[0.95][1](cnn_miss_loc_preds[:,[0,3,4]])]).transpose()\n",
    "missNumLoc_delta_95_q = np.array([delta_cpi_fun[0.95][0](cnn_miss_loc_preds[:,[1,3,4]]), delta_cpi_fun[0.95][1](cnn_miss_loc_preds[:,[1,3,4]])]).transpose()\n",
    "missNumLoc_m_95_q = np.array([m_cpi_fun[0.95][0](cnn_miss_loc_preds[:,[2,3,4]]), m_cpi_fun[0.95][1](cnn_miss_loc_preds[:,[2,3,4]])]).transpose()\n",
    "\n",
    "df_cnn_95q = pd.DataFrame(np.hstack((missNumLoc_R0_95_q, missNumLoc_delta_95_q, missNumLoc_m_95_q)),\n",
    "                         columns = [\"R0_lq\", \"R0_uq\", \"delta_lq\", \"delta_uq\", \"m_lq\", \"m_uq\"])\n",
    "df_cnn_95q.to_csv('output/missNumLoc_cnn_95q.tsv', sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "311b8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 2.9702970297029703\n",
      "0.1 finished: 4.9504950495049505\n",
      "0.25 finished: 18.81188118811881\n",
      "0.5 finished: 31.683168316831683\n",
      "0.75 finished: 54.45544554455446\n",
      "0.9 finished: 70.29702970297029\n",
      "0.95 finished: 77.22772277227723\n",
      "0.05 finished: 3.9603960396039604\n",
      "0.1 finished: 5.9405940594059405\n",
      "0.25 finished: 16.831683168316832\n",
      "0.5 finished: 35.64356435643564\n",
      "0.75 finished: 53.46534653465347\n",
      "0.9 finished: 68.31683168316832\n",
      "0.95 finished: 77.22772277227723\n",
      "0.05 finished: 5.9405940594059405\n",
      "0.1 finished: 9.900990099009901\n",
      "0.25 finished: 21.782178217821784\n",
      "0.5 finished: 39.603960396039604\n",
      "0.75 finished: 66.33663366336634\n",
      "0.9 finished: 86.13861386138613\n",
      "0.95 finished: 91.0891089108911\n"
     ]
    }
   ],
   "source": [
    "# make coverage sets for tree misspecification experiments\n",
    "\n",
    "cnn_miss_tree_preds = pd.read_table(\"../output/misspec_tree_cnn_preds.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_tree_labels = pd.read_table(\"../output/misspec_tree_labels.tsv\", header = 0).to_numpy()\n",
    "cnn_miss_tree_mutips = pd.read_table(\"data_files/labels_and_preds/extant_misspec_tree_param_values_numtips_propsamp.txt\", \n",
    "                                     header = 0).to_numpy()\n",
    "\n",
    "cnn_miss_tree_preds = np.column_stack((np.log(cnn_miss_tree_preds[:,0:3]), cnn_miss_tree_mutips[:,0], \n",
    "                                       cnn_miss_tree_mutips[:,1]/cnn_miss_tree_mutips[:,2]))\n",
    "\n",
    "i=0\n",
    "missTree_R0_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_tree_preds[:,[i,3,4]], cnn_miss_tree_labels[:,i], q_fun = R0_cpi_fun)\n",
    "i=1\n",
    "missTree_delta_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_tree_preds[:,[i,3,4]], cnn_miss_tree_labels[:,i], q_fun = delta_cpi_fun)\n",
    "i=2\n",
    "missTree_m_coverage = uq.make_coverage_set(None, None, \n",
    "                                   cnn_miss_tree_preds[:,[i,3,4]], cnn_miss_tree_labels[:,i], q_fun = m_cpi_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "ea2bdafc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 finished: 2.9702970297029703\n",
      "0.1 finished: 2.9702970297029703\n",
      "0.25 finished: 20.792079207920793\n",
      "0.5 finished: 31.683168316831683\n",
      "0.75 finished: 54.45544554455446\n",
      "0.9 finished: 72.27722772277228\n",
      "0.95 finished: 78.21782178217822\n",
      "0.05 finished: 4.9504950495049505\n",
      "0.1 finished: 5.9405940594059405\n",
      "0.25 finished: 16.831683168316832\n",
      "0.5 finished: 34.65346534653465\n",
      "0.75 finished: 53.46534653465347\n",
      "0.9 finished: 69.3069306930693\n",
      "0.95 finished: 79.20792079207921\n",
      "0.05 finished: 5.9405940594059405\n",
      "0.1 finished: 9.900990099009901\n",
      "0.25 finished: 19.801980198019802\n",
      "0.5 finished: 38.613861386138616\n",
      "0.75 finished: 67.32673267326733\n",
      "0.9 finished: 87.12871287128714\n",
      "0.95 finished: 92.07920792079209\n"
     ]
    }
   ],
   "source": [
    "df_missTree_cnn_coverage = pd.DataFrame(np.transpose(np.vstack((missTree_R0_coverage, missTree_delta_coverage, missTree_m_coverage))), \n",
    "                                            columns = [\"R0\", \"sample_rate\", \"migration_rate\"], \n",
    "                               index = [\"5\", \"10\", \"25\", \"50\", \"75\", \"90\", \"95\"])\n",
    "df_missTree_cnn_coverage.to_csv('../data_files/missTree_cnn_coverage.tsv', sep = \"\\t\", index = True)\n",
    "\n",
    "# write 95% quantiles to file\n",
    "missTree_R0_95_q = np.array([R0_cpi_fun[0.95][0](cnn_miss_tree_preds[:,[0,3,4]]), R0_cpi_fun[0.95][1](cnn_miss_tree_preds[:,[0,3,4]])]).transpose()\n",
    "missTree_delta_95_q = np.array([delta_cpi_fun[0.95][0](cnn_miss_tree_preds[:,[1,3,4]]), delta_cpi_fun[0.95][1](cnn_miss_tree_preds[:,[1,3,4]])]).transpose()\n",
    "missTree_m_95_q = np.array([m_cpi_fun[0.95][0](cnn_miss_tree_preds[:,[2,3,4]]), m_cpi_fun[0.95][1](cnn_miss_tree_preds[:,[2,3,4]])]).transpose()\n",
    "\n",
    "df_cnn_95q = pd.DataFrame(np.hstack((missTree_R0_95_q, missTree_delta_95_q, missTree_m_95_q)),\n",
    "                         columns = [\"R0_lq\", \"R0_uq\", \"delta_lq\", \"delta_uq\", \"m_lq\", \"m_uq\"])\n",
    "df_cnn_95q.to_csv('output/missTree_cnn_95q.tsv', sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92c359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdae42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
